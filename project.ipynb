{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.bow_vectorizer = CountVectorizer(max_features = 5000)\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features = 5000)\n",
    "        # pass\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        if isinstance(text, float):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        # https://medium.com/@siddharthgov01/regular-expressions-from-a-za-z-88cf9cf0abac\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        return text\n",
    "    \n",
    "    def train_bow_vectorizer(self, train_texts):\n",
    "        self.bow_vectorizer.fit(train_texts)\n",
    "    \n",
    "    def train_tfidf_vectorizer(self, train_texts):\n",
    "        self.tfidf_vectorizer.fit(train_texts)\n",
    "    \n",
    "\n",
    "    # https://pages.github.rpi.edu/kuruzj/website_introml_rpi/notebooks/08-intro-nlp/03-scikit-learn-text.html#bag-of-words-using-scikit-learn\n",
    "    def get_bow_features(self, texts, max_features=5000):\n",
    "        return self.bow_vectorizer.transform(texts)\n",
    "        # vectorizer = CountVectorizer(max_features=max_features)\n",
    "        # return vectorizer.fit_transform(texts)\n",
    "\n",
    "    # https://pages.github.rpi.edu/kuruzj/website_introml_rpi/notebooks/08-intro-nlp/03-scikit-learn-text.html#tf-idf-encoding\n",
    "    def get_tfidf_features(self, texts, max_features=5000):\n",
    "        return self.tfidf_vectorizer.transform(texts)\n",
    "        # vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "        # return vectorizer.fit_transform(texts)\n",
    " \n",
    "    # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    def get_word2vec_features(self, texts, vector_size=100, window=5, min_count=1, workers=4):\n",
    "        processed_texts = []\n",
    "        for text in texts:\n",
    "            if isinstance(text, str):\n",
    "                cleaned = self.clean_text(text)\n",
    "                tokens = cleaned.split()\n",
    "                processed_texts.append(tokens)\n",
    "\n",
    "        model = Word2Vec(\n",
    "            sentences=processed_texts,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            workers=workers\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def get_text_vector(self, text, word2vec_model):\n",
    "        tokens = self.clean_text(text).split()\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in word2vec_model.wv:\n",
    "                vectors.append(word2vec_model.wv[token])\n",
    "                \n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with TF-IDF Word Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number Iterations: 6\n",
      "Train Accuracy: 98.94%\n",
      "Test Accuracy: 94.25%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      2077\n",
      "           1       0.94      0.94      0.94      2083\n",
      "\n",
      "    accuracy                           0.94      4160\n",
      "   macro avg       0.94      0.94      0.94      4160\n",
      "weighted avg       0.94      0.94      0.94      4160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "scaler = StandardScaler()\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "preprocessor.train_tfidf_vectorizer(X_train)\n",
    "X_train_tfidf = preprocessor.get_tfidf_features(X_train)\n",
    "X_test_tfidf = preprocessor.get_tfidf_features(X_test)\n",
    "\n",
    "X_train_tfidf_scaled = scaler.fit_transform(X = pd.DataFrame(X_train_tfidf.todense()))\n",
    "X_test_tfidf_scaled = scaler.transform(X = pd.DataFrame(X_test_tfidf.todense()))\n",
    "best_iteration = None\n",
    "best_train_accuracy = None\n",
    "best_test_accuracy = None\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for i in range(1, 51):\n",
    "    lr_classifier = LogisticRegression(max_iter=i, C=5.0, penalty='l2', random_state=42 )\n",
    "\n",
    "\n",
    "    lr_classifier.fit(X_train_tfidf_scaled, y_train)\n",
    "\n",
    "\n",
    "    y_train_pred = lr_classifier.predict(X_train_tfidf_scaled)\n",
    "    y_test_pred = lr_classifier.predict(X_test_tfidf_scaled)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    if best_test_accuracy == None or test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        best_train_accuracy = train_accuracy\n",
    "        best_iteration = i\n",
    "\n",
    "print(\"Best Number Iterations:\", best_iteration)\n",
    "print(f\"Train Accuracy: {best_train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {best_test_accuracy * 100:.2f}%\")\n",
    "\n",
    "lr_classifier = LogisticRegression(max_iter=best_iteration, C=5.0, penalty='l2', random_state=42 )\n",
    "lr_classifier.fit(X_train_tfidf_scaled, y_train)\n",
    "y_test_pred = lr_classifier.predict(X_test_tfidf_scaled)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "pd.DataFrame({\"train accuracy\": train_accuracies, \"test accuracy\": test_accuracies}).to_csv(\"./results/logistic_regression_tfidf.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number Iterations: 10\n",
      "Train Accuracy: 99.22%\n",
      "Test Accuracy: 94.28%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94      2077\n",
      "           1       0.93      0.96      0.94      2083\n",
      "\n",
      "    accuracy                           0.94      4160\n",
      "   macro avg       0.94      0.94      0.94      4160\n",
      "weighted avg       0.94      0.94      0.94      4160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "preprocessor.train_bow_vectorizer(X_train)\n",
    "X_train_bow = preprocessor.get_bow_features(X_train)\n",
    "X_test_bow = preprocessor.get_bow_features (X_test)\n",
    "\n",
    "X_train_bow_scaled = scaler.fit_transform(X = pd.DataFrame(X_train_bow.todense()))\n",
    "X_test_bow_scaled = scaler.transform(X = pd.DataFrame(X_test_bow.todense()))\n",
    "\n",
    "best_iteration = None\n",
    "best_train_accuracy = None\n",
    "best_test_accuracy = None\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for i in range(1, 51):\n",
    "    lr_classifier = LogisticRegression(max_iter=i, C=5.0, penalty='l2', random_state=42 )\n",
    "\n",
    "\n",
    "    lr_classifier.fit(X_train_bow_scaled, y_train)\n",
    "\n",
    "\n",
    "    y_train_pred = lr_classifier.predict(X_train_bow_scaled)\n",
    "    y_test_pred = lr_classifier.predict(X_test_bow_scaled)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    if best_test_accuracy == None or test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        best_train_accuracy = train_accuracy\n",
    "        best_iteration = i\n",
    "\n",
    "print(\"Best Number Iterations:\", best_iteration)\n",
    "print(f\"Train Accuracy: {best_train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {best_test_accuracy * 100:.2f}%\") \n",
    "\n",
    "lr_classifier = LogisticRegression(max_iter=best_iteration, C=5.0, penalty='l2', random_state=42 )\n",
    "\n",
    "lr_classifier.fit(X_train_bow_scaled, y_train)\n",
    "y_test_pred = lr_classifier.predict(X_test_bow_scaled)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "pd.DataFrame({\"train accuracy\": train_accuracies, \"test accuracy\": test_accuracies}).to_csv(\"./results/logistic_regression_bow.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Word2vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 89.84%\n",
      "Test Accuracy: 88.51%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      2077\n",
      "           1       0.89      0.88      0.88      2083\n",
      "\n",
      "    accuracy                           0.89      4160\n",
      "   macro avg       0.89      0.89      0.89      4160\n",
      "weighted avg       0.89      0.89      0.89      4160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "X_train_text_vectors = []\n",
    "X_test_text_vectors = []\n",
    "\n",
    "word2vec_model = preprocessor.get_word2vec_features(X_train)\n",
    "\n",
    "for text in X_train:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_train_text_vectors.append(text_vector)\n",
    "    \n",
    "for text in X_test:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_test_text_vectors.append(text_vector)\n",
    "\n",
    "X_train_text_vectors = pd.DataFrame(X_train_text_vectors)\n",
    "X_test_text_vectors = pd.DataFrame(X_test_text_vectors)\n",
    "\n",
    "lr_classifier = LogisticRegression(max_iter=1000, C=5.0, penalty='l2', random_state=42)\n",
    "\n",
    "lr_classifier.fit(X_train_text_vectors, y_train)\n",
    "\n",
    "y_train_pred = lr_classifier.predict(X_train_text_vectors)\n",
    "y_test_pred = lr_classifier.predict(X_test_text_vectors)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.8238 - loss: 0.4092 - val_accuracy: 0.9177 - val_loss: 0.2172\n",
      "Epoch 2/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9392 - loss: 0.1714 - val_accuracy: 0.9177 - val_loss: 0.2253\n",
      "Epoch 3/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9508 - loss: 0.1472 - val_accuracy: 0.9225 - val_loss: 0.2211\n",
      "Epoch 4/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.9526 - loss: 0.1391 - val_accuracy: 0.9138 - val_loss: 0.2351\n",
      "Epoch 5/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.9519 - loss: 0.1344 - val_accuracy: 0.9261 - val_loss: 0.2231\n",
      "Epoch 6/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9564 - loss: 0.1305 - val_accuracy: 0.9210 - val_loss: 0.2185\n",
      "Epoch 7/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9543 - loss: 0.1372 - val_accuracy: 0.9258 - val_loss: 0.2187\n",
      "Epoch 8/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9594 - loss: 0.1342 - val_accuracy: 0.9231 - val_loss: 0.2284\n",
      "Epoch 9/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9550 - loss: 0.1371 - val_accuracy: 0.9150 - val_loss: 0.2279\n",
      "Epoch 10/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9551 - loss: 0.1395 - val_accuracy: 0.9177 - val_loss: 0.2314\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Accuracy: 0.9242788461538461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      2077\n",
      "           1       0.91      0.94      0.93      2083\n",
      "\n",
      "    accuracy                           0.92      4160\n",
      "   macro avg       0.92      0.92      0.92      4160\n",
      "weighted avg       0.92      0.92      0.92      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout,BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "preprocessor.train_bow_vectorizer(X_train)\n",
    "X_train_bow = preprocessor.get_bow_features(X_train)\n",
    "X_test_bow = preprocessor.get_bow_features(X_test)\n",
    "\n",
    "X_train_bow_scaled = scaler.fit_transform(X = pd.DataFrame(X_train_bow.todense()))\n",
    "X_test_bow_scaled = scaler.transform(X = pd.DataFrame(X_test_bow.todense()))\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train_bow_scaled.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer= \"Adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "training = model.fit(\n",
    "    X_train_bow_scaled,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_bow_scaled)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n",
    "\n",
    "pd.DataFrame(training.history).to_csv(\"./results/mlp_bow.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model with TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.8285 - loss: 0.3864 - val_accuracy: 0.9270 - val_loss: 0.1853\n",
      "Epoch 2/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9551 - loss: 0.1236 - val_accuracy: 0.9240 - val_loss: 0.2006\n",
      "Epoch 3/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9620 - loss: 0.1023 - val_accuracy: 0.9318 - val_loss: 0.1908\n",
      "Epoch 4/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9679 - loss: 0.0894 - val_accuracy: 0.9246 - val_loss: 0.2061\n",
      "Epoch 5/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9694 - loss: 0.0889 - val_accuracy: 0.9258 - val_loss: 0.2137\n",
      "Epoch 6/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9754 - loss: 0.0686 - val_accuracy: 0.9294 - val_loss: 0.2230\n",
      "Epoch 7/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9731 - loss: 0.0702 - val_accuracy: 0.9267 - val_loss: 0.2248\n",
      "Epoch 8/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9724 - loss: 0.0735 - val_accuracy: 0.9276 - val_loss: 0.2185\n",
      "Epoch 9/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9748 - loss: 0.0666 - val_accuracy: 0.9204 - val_loss: 0.2412\n",
      "Epoch 10/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9727 - loss: 0.0716 - val_accuracy: 0.9267 - val_loss: 0.2176\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Accuracy: 0.9213942307692308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      2077\n",
      "           1       0.93      0.91      0.92      2083\n",
      "\n",
      "    accuracy                           0.92      4160\n",
      "   macro avg       0.92      0.92      0.92      4160\n",
      "weighted avg       0.92      0.92      0.92      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout,BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "preprocessor.train_tfidf_vectorizer(X_train)\n",
    "X_train_tfidf = preprocessor.get_tfidf_features(X_train)\n",
    "X_test_tfidf = preprocessor.get_tfidf_features(X_test)\n",
    "\n",
    "X_train_tfidf_scaled = scaler.fit_transform(X = pd.DataFrame(X_train_tfidf.todense()))\n",
    "X_test_tfidf_scaled = scaler.transform(X = pd.DataFrame(X_test_tfidf.todense()))\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train_tfidf_scaled.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "training = model.fit(\n",
    "    X_train_tfidf_scaled,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_tfidf_scaled)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n",
    "\n",
    "pd.DataFrame(training.history).to_csv(\"./results/mlp_tfidf.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.7840 - loss: 0.4904 - val_accuracy: 0.8606 - val_loss: 0.3326\n",
      "Epoch 2/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8565 - loss: 0.3398 - val_accuracy: 0.8657 - val_loss: 0.3136\n",
      "Epoch 3/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8583 - loss: 0.3319 - val_accuracy: 0.8594 - val_loss: 0.3213\n",
      "Epoch 4/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8618 - loss: 0.3231 - val_accuracy: 0.8492 - val_loss: 0.3298\n",
      "Epoch 5/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8636 - loss: 0.3283 - val_accuracy: 0.8612 - val_loss: 0.3220\n",
      "Epoch 6/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8556 - loss: 0.3389 - val_accuracy: 0.8777 - val_loss: 0.3054\n",
      "Epoch 7/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8632 - loss: 0.3206 - val_accuracy: 0.8747 - val_loss: 0.3054\n",
      "Epoch 8/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8562 - loss: 0.3312 - val_accuracy: 0.8738 - val_loss: 0.3064\n",
      "Epoch 9/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8666 - loss: 0.3191 - val_accuracy: 0.8759 - val_loss: 0.3060\n",
      "Epoch 10/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8594 - loss: 0.3313 - val_accuracy: 0.8810 - val_loss: 0.3047\n",
      "Epoch 11/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8640 - loss: 0.3234 - val_accuracy: 0.8756 - val_loss: 0.3052\n",
      "Epoch 12/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8687 - loss: 0.3164 - val_accuracy: 0.8792 - val_loss: 0.3020\n",
      "Epoch 13/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8627 - loss: 0.3263 - val_accuracy: 0.8762 - val_loss: 0.3136\n",
      "Epoch 14/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8600 - loss: 0.3323 - val_accuracy: 0.8771 - val_loss: 0.2981\n",
      "Epoch 15/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.8628 - loss: 0.3260 - val_accuracy: 0.8819 - val_loss: 0.3054\n",
      "Epoch 16/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8598 - loss: 0.3256 - val_accuracy: 0.8795 - val_loss: 0.3004\n",
      "Epoch 17/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8625 - loss: 0.3235 - val_accuracy: 0.8732 - val_loss: 0.3074\n",
      "Epoch 18/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8579 - loss: 0.3293 - val_accuracy: 0.8762 - val_loss: 0.3084\n",
      "Epoch 19/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.8634 - loss: 0.3225 - val_accuracy: 0.8573 - val_loss: 0.3392\n",
      "Epoch 20/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.8645 - loss: 0.3250 - val_accuracy: 0.8798 - val_loss: 0.2990\n",
      "Epoch 21/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8631 - loss: 0.3290 - val_accuracy: 0.8795 - val_loss: 0.3052\n",
      "Epoch 22/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8657 - loss: 0.3292 - val_accuracy: 0.8729 - val_loss: 0.3100\n",
      "Epoch 23/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.8609 - loss: 0.3252 - val_accuracy: 0.8822 - val_loss: 0.3046\n",
      "Epoch 24/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.8671 - loss: 0.3229 - val_accuracy: 0.8807 - val_loss: 0.3053\n",
      "Epoch 25/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8575 - loss: 0.3327 - val_accuracy: 0.8813 - val_loss: 0.3014\n",
      "Epoch 26/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8601 - loss: 0.3274 - val_accuracy: 0.8774 - val_loss: 0.2983\n",
      "Epoch 27/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8625 - loss: 0.3283 - val_accuracy: 0.8804 - val_loss: 0.2987\n",
      "Epoch 28/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.8636 - loss: 0.3174 - val_accuracy: 0.8813 - val_loss: 0.3067\n",
      "Epoch 29/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.8640 - loss: 0.3205 - val_accuracy: 0.8798 - val_loss: 0.3019\n",
      "Epoch 30/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8701 - loss: 0.3220 - val_accuracy: 0.8816 - val_loss: 0.3021\n",
      "Epoch 31/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8546 - loss: 0.3335 - val_accuracy: 0.8798 - val_loss: 0.3011\n",
      "Epoch 32/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8658 - loss: 0.3217 - val_accuracy: 0.8780 - val_loss: 0.3000\n",
      "Epoch 33/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.8721 - loss: 0.3161 - val_accuracy: 0.8774 - val_loss: 0.3070\n",
      "Epoch 34/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8598 - loss: 0.3290 - val_accuracy: 0.8783 - val_loss: 0.2998\n",
      "Epoch 35/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.8592 - loss: 0.3317 - val_accuracy: 0.8807 - val_loss: 0.2984\n",
      "Epoch 36/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8663 - loss: 0.3173 - val_accuracy: 0.8813 - val_loss: 0.3006\n",
      "Epoch 37/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.8651 - loss: 0.3246 - val_accuracy: 0.8801 - val_loss: 0.3003\n",
      "Epoch 38/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.8630 - loss: 0.3288 - val_accuracy: 0.8798 - val_loss: 0.3003\n",
      "Epoch 39/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.8643 - loss: 0.3215 - val_accuracy: 0.8738 - val_loss: 0.3097\n",
      "Epoch 40/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8657 - loss: 0.3197 - val_accuracy: 0.8591 - val_loss: 0.3321\n",
      "Epoch 41/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8566 - loss: 0.3250 - val_accuracy: 0.8762 - val_loss: 0.3004\n",
      "Epoch 42/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8611 - loss: 0.3278 - val_accuracy: 0.8789 - val_loss: 0.3042\n",
      "Epoch 43/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8678 - loss: 0.3248 - val_accuracy: 0.8729 - val_loss: 0.3085\n",
      "Epoch 44/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.8654 - loss: 0.3211 - val_accuracy: 0.8786 - val_loss: 0.3038\n",
      "Epoch 45/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8660 - loss: 0.3220 - val_accuracy: 0.8789 - val_loss: 0.3014\n",
      "Epoch 46/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8627 - loss: 0.3333 - val_accuracy: 0.8810 - val_loss: 0.3003\n",
      "Epoch 47/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8614 - loss: 0.3248 - val_accuracy: 0.8816 - val_loss: 0.3048\n",
      "Epoch 48/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.8639 - loss: 0.3289 - val_accuracy: 0.8783 - val_loss: 0.3030\n",
      "Epoch 49/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8639 - loss: 0.3199 - val_accuracy: 0.8729 - val_loss: 0.3218\n",
      "Epoch 50/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8695 - loss: 0.3168 - val_accuracy: 0.8765 - val_loss: 0.3070\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Accuracy: 0.8771634615384616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88      2077\n",
      "           1       0.90      0.84      0.87      2083\n",
      "\n",
      "    accuracy                           0.88      4160\n",
      "   macro avg       0.88      0.88      0.88      4160\n",
      "weighted avg       0.88      0.88      0.88      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "X_train_text_vectors = []\n",
    "X_test_text_vectors = []\n",
    "\n",
    "word2vec_model = preprocessor.get_word2vec_features(X_train)\n",
    "\n",
    "for text in X_train:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_train_text_vectors.append(text_vector)\n",
    "    \n",
    "for text in X_test:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_test_text_vectors.append(text_vector)\n",
    "\n",
    "X_train_text_vectors = pd.DataFrame(X_train_text_vectors)\n",
    "X_test_text_vectors = pd.DataFrame(X_test_text_vectors)\n",
    "\n",
    "X_train_word2Vec = X_train_text_vectors.to_numpy()\n",
    "X_test_word2Vec = X_test_text_vectors.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train_word2Vec.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train_word2Vec,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_word2Vec) \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 460ms/step - accuracy: 0.7829 - loss: 0.4229 - val_accuracy: 0.9303 - val_loss: 0.1759\n",
      "Epoch 2/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 410ms/step - accuracy: 0.9734 - loss: 0.0802 - val_accuracy: 0.9351 - val_loss: 0.1999\n",
      "Epoch 3/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 384ms/step - accuracy: 0.9950 - loss: 0.0182 - val_accuracy: 0.9264 - val_loss: 0.2844\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step\n",
      "Accuracy: 0.9305288461538461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93      2077\n",
      "           1       0.97      0.89      0.93      2083\n",
      "\n",
      "    accuracy                           0.93      4160\n",
      "   macro avg       0.93      0.93      0.93      4160\n",
      "weighted avg       0.93      0.93      0.93      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "tokenizer = Tokenizer()\n",
    "max_sequence_length = 200\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "X_train_sequences = pd.DataFrame(X_train_sequences)\n",
    "X_test_sequences = pd.DataFrame(X_test_sequences)\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))  \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train_sequences,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_sequences)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 120ms/step - accuracy: 0.7196 - loss: 0.5299 - val_accuracy: 0.8915 - val_loss: 0.2555\n",
      "Epoch 2/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 118ms/step - accuracy: 0.9244 - loss: 0.2158 - val_accuracy: 0.9053 - val_loss: 0.2363\n",
      "Epoch 3/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 119ms/step - accuracy: 0.9759 - loss: 0.0871 - val_accuracy: 0.9198 - val_loss: 0.2324\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step\n",
      "Accuracy: 0.9225961538461539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92      2077\n",
      "           1       0.94      0.90      0.92      2083\n",
      "\n",
      "    accuracy                           0.92      4160\n",
      "   macro avg       0.92      0.92      0.92      4160\n",
      "weighted avg       0.92      0.92      0.92      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "tokenizer = Tokenizer()\n",
    "max_sequence_length = 200\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "X_train_sequences = pd.DataFrame(X_train_sequences)\n",
    "X_test_sequences = pd.DataFrame(X_test_sequences)\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(5, return_sequences=False)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dropout(0.3))  \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer = \"adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train_sequences,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_sequences)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train.tolist(), truncation = True, padding = True, max_length = 300, return_tensors = \"tf\")\n",
    "X_test_tokenized = tokenizer(X_test.tolist(), truncation = True, padding = True, max_length = 300, return_tensors = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "416/416 [==============================] - 462s 1s/step - loss: 0.6809 - accuracy: 0.5621 - val_loss: 0.6707 - val_accuracy: 0.5913\n",
      "Epoch 2/5\n",
      "416/416 [==============================] - 441s 1s/step - loss: 0.6731 - accuracy: 0.5750 - val_loss: 0.6697 - val_accuracy: 0.5895\n",
      "Epoch 3/5\n",
      "416/416 [==============================] - 448s 1s/step - loss: 0.6694 - accuracy: 0.5814 - val_loss: 0.6642 - val_accuracy: 0.5959\n",
      "Epoch 4/5\n",
      "416/416 [==============================] - 444s 1s/step - loss: 0.6667 - accuracy: 0.5877 - val_loss: 0.6639 - val_accuracy: 0.5974\n",
      "Epoch 5/5\n",
      "416/416 [==============================] - 441s 1s/step - loss: 0.6642 - accuracy: 0.5915 - val_loss: 0.6615 - val_accuracy: 0.5974\n",
      "130/130 [==============================] - 102s 748ms/step\n",
      "Results on Test Data:\n",
      "Accuracy: 0.6045673076923077\n",
      "Precision: 0.6518810299712648\n",
      "Recall: 0.6049683211711563\n",
      "F1 Score: 0.5716626071884093\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.88      0.69      2077\n",
      "           1       0.74      0.33      0.45      2083\n",
      "\n",
      "    accuracy                           0.60      4160\n",
      "   macro avg       0.65      0.60      0.57      4160\n",
      "weighted avg       0.65      0.60      0.57      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from transformers import AdamWeightDecay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Initialize the Hugging Face model\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "for layer in model.layers:\n",
    "    if layer.name != \"classifier\":\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamWeightDecay(learning_rate=1e-5, weight_decay_rate=0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "# Train the model\n",
    "training = model.fit(\n",
    "    X_train_tokenized,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "y_test_pred_logits = model.predict(X_test_tokenized).logits\n",
    "y_test_pred = np.argmax(y_test_pred_logits, axis = 1)  \n",
    "print(\"Results on Test Data:\")\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print()\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "pd.DataFrame(training.history).to_csv(\"./results/bert.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x39efd65e0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x39efd65e0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "416/416 [==============================] - 1085s 3s/step - loss: 0.3111 - accuracy: 0.8645 - val_loss: 0.1069 - val_accuracy: 0.9609\n",
      "Epoch 2/5\n",
      "416/416 [==============================] - 1050s 3s/step - loss: 0.0997 - accuracy: 0.9651 - val_loss: 0.0750 - val_accuracy: 0.9769\n",
      "Epoch 3/5\n",
      "416/416 [==============================] - 1042s 3s/step - loss: 0.0620 - accuracy: 0.9803 - val_loss: 0.0727 - val_accuracy: 0.9763\n",
      "Epoch 4/5\n",
      "416/416 [==============================] - 1042s 3s/step - loss: 0.0404 - accuracy: 0.9865 - val_loss: 0.0655 - val_accuracy: 0.9811\n",
      "Epoch 5/5\n",
      "416/416 [==============================] - 1041s 3s/step - loss: 0.0335 - accuracy: 0.9889 - val_loss: 0.0702 - val_accuracy: 0.9775\n",
      "130/130 [==============================] - 100s 740ms/step\n",
      "Results on Test Data:\n",
      "Accuracy: 0.9814903846153846\n",
      "Precision: 0.9815049811617318\n",
      "Recall: 0.9814948533315644\n",
      "F1 Score: 0.9814903322061228\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      2077\n",
      "           1       0.98      0.98      0.98      2083\n",
      "\n",
      "    accuracy                           0.98      4160\n",
      "   macro avg       0.98      0.98      0.98      4160\n",
      "weighted avg       0.98      0.98      0.98      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from transformers import AdamWeightDecay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Initialize the Hugging Face model\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Freeze 6 layers of encoder\n",
    "for layer in model.bert.encoder.layer[0:6]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamWeightDecay(learning_rate=1e-5, weight_decay_rate=0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "# Train the model\n",
    "training = model.fit(\n",
    "    X_train_tokenized,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "y_test_pred_logits = model.predict(X_test_tokenized).logits\n",
    "y_test_pred = np.argmax(y_test_pred_logits, axis = 1)  \n",
    "print(\"Results on Test Data:\")\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print()\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "pd.DataFrame(training.history).to_csv(\"./results/bert2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 97s 749ms/step\n",
      "Results on Test Data:\n",
      "Accuracy: 0.4206730769230769\n",
      "Precision: 0.3942262643853256\n",
      "Recall: 0.4203147611947233\n",
      "F1 Score: 0.38236740760892596\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.17      0.23      2077\n",
      "           1       0.45      0.67      0.54      2083\n",
      "\n",
      "    accuracy                           0.42      4160\n",
      "   macro avg       0.39      0.42      0.38      4160\n",
      "weighted avg       0.39      0.42      0.38      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "y_test_pred_logits = model.predict(X_test_tokenized).logits\n",
    "y_test_pred = np.argmax(y_test_pred_logits, axis = 1)  \n",
    "print(\"Results on Test Data:\")\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print()\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
