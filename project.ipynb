{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.bow_vectorizer = CountVectorizer(max_features = 5000)\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features = 5000)\n",
    "        # pass\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        if isinstance(text, float):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        # https://medium.com/@siddharthgov01/regular-expressions-from-a-za-z-88cf9cf0abac\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        return text\n",
    "    \n",
    "    def train_bow_vectorizer(self, train_texts):\n",
    "        self.bow_vectorizer.fit(train_texts)\n",
    "    \n",
    "    def train_tfidf_vectorizer(self, train_texts):\n",
    "        self.tfidf_vectorizer.fit(train_texts)\n",
    "    \n",
    "\n",
    "    # https://pages.github.rpi.edu/kuruzj/website_introml_rpi/notebooks/08-intro-nlp/03-scikit-learn-text.html#bag-of-words-using-scikit-learn\n",
    "    def get_bow_features(self, texts, max_features=5000):\n",
    "        return self.bow_vectorizer.transform(texts)\n",
    "        # vectorizer = CountVectorizer(max_features=max_features)\n",
    "        # return vectorizer.fit_transform(texts)\n",
    "\n",
    "    # https://pages.github.rpi.edu/kuruzj/website_introml_rpi/notebooks/08-intro-nlp/03-scikit-learn-text.html#tf-idf-encoding\n",
    "    def get_tfidf_features(self, texts, max_features=5000):\n",
    "        return self.tfidf_vectorizer.transform(texts)\n",
    "        # vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "        # return vectorizer.fit_transform(texts)\n",
    " \n",
    "    # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    def get_word2vec_features(self, texts, vector_size=100, window=5, min_count=1, workers=4):\n",
    "        processed_texts = []\n",
    "        for text in texts:\n",
    "            if isinstance(text, str):\n",
    "                cleaned = self.clean_text(text)\n",
    "                tokens = cleaned.split()\n",
    "                processed_texts.append(tokens)\n",
    "\n",
    "        model = Word2Vec(\n",
    "            sentences=processed_texts,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            workers=workers\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def get_text_vector(self, text, word2vec_model):\n",
    "        tokens = self.clean_text(text).split()\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in word2vec_model.wv:\n",
    "                vectors.append(word2vec_model.wv[token])\n",
    "                \n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with TF-IDF Word Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number Iterations: 6\n",
      "Train Accuracy: 98.94%\n",
      "Test Accuracy: 94.25%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      2077\n",
      "           1       0.94      0.94      0.94      2083\n",
      "\n",
      "    accuracy                           0.94      4160\n",
      "   macro avg       0.94      0.94      0.94      4160\n",
      "weighted avg       0.94      0.94      0.94      4160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "scaler = StandardScaler()\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "preprocessor.train_tfidf_vectorizer(X_train)\n",
    "X_train_tfidf = preprocessor.get_tfidf_features(X_train)\n",
    "X_test_tfidf = preprocessor.get_tfidf_features(X_test)\n",
    "\n",
    "X_train_tfidf_scaled = scaler.fit_transform(X = pd.DataFrame(X_train_tfidf.todense()))\n",
    "X_test_tfidf_scaled = scaler.transform(X = pd.DataFrame(X_test_tfidf.todense()))\n",
    "best_iteration = None\n",
    "best_train_accuracy = None\n",
    "best_test_accuracy = None\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for i in range(1, 51):\n",
    "    lr_classifier = LogisticRegression(max_iter=i, C=5.0, penalty='l2', random_state=42 )\n",
    "\n",
    "\n",
    "    lr_classifier.fit(X_train_tfidf_scaled, y_train)\n",
    "\n",
    "\n",
    "    y_train_pred = lr_classifier.predict(X_train_tfidf_scaled)\n",
    "    y_test_pred = lr_classifier.predict(X_test_tfidf_scaled)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    if best_test_accuracy == None or test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        best_train_accuracy = train_accuracy\n",
    "        best_iteration = i\n",
    "\n",
    "print(\"Best Number Iterations:\", best_iteration)\n",
    "print(f\"Train Accuracy: {best_train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {best_test_accuracy * 100:.2f}%\")\n",
    "\n",
    "lr_classifier = LogisticRegression(max_iter=best_iteration, C=5.0, penalty='l2', random_state=42 )\n",
    "lr_classifier.fit(X_train_tfidf_scaled, y_train)\n",
    "y_test_pred = lr_classifier.predict(X_test_tfidf_scaled)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "pd.DataFrame({\"train accuracy\": train_accuracies, \"test accuracy\": test_accuracies}).to_csv(\"./results/linear_regression_tfidf.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number Iterations: 10\n",
      "Train Accuracy: 99.22%\n",
      "Test Accuracy: 94.28%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94      2077\n",
      "           1       0.93      0.96      0.94      2083\n",
      "\n",
      "    accuracy                           0.94      4160\n",
      "   macro avg       0.94      0.94      0.94      4160\n",
      "weighted avg       0.94      0.94      0.94      4160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "preprocessor.train_bow_vectorizer(X_train)\n",
    "X_train_bow = preprocessor.get_bow_features(X_train)\n",
    "X_test_bow = preprocessor.get_bow_features (X_test)\n",
    "\n",
    "X_train_bow_scaled = scaler.fit_transform(X = pd.DataFrame(X_train_bow.todense()))\n",
    "X_test_bow_scaled = scaler.transform(X = pd.DataFrame(X_test_bow.todense()))\n",
    "\n",
    "best_iteration = None\n",
    "best_train_accuracy = None\n",
    "best_test_accuracy = None\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for i in range(1, 51):\n",
    "    lr_classifier = LogisticRegression(max_iter=i, C=5.0, penalty='l2', random_state=42 )\n",
    "\n",
    "\n",
    "    lr_classifier.fit(X_train_bow_scaled, y_train)\n",
    "\n",
    "\n",
    "    y_train_pred = lr_classifier.predict(X_train_bow_scaled)\n",
    "    y_test_pred = lr_classifier.predict(X_test_bow_scaled)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    if best_test_accuracy == None or test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        best_train_accuracy = train_accuracy\n",
    "        best_iteration = i\n",
    "\n",
    "print(\"Best Number Iterations:\", best_iteration)\n",
    "print(f\"Train Accuracy: {best_train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {best_test_accuracy * 100:.2f}%\") \n",
    "\n",
    "lr_classifier = LogisticRegression(max_iter=best_iteration, C=5.0, penalty='l2', random_state=42 )\n",
    "\n",
    "lr_classifier.fit(X_train_bow_scaled, y_train)\n",
    "y_test_pred = lr_classifier.predict(X_test_bow_scaled)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "pd.DataFrame({\"train accuracy\": train_accuracies, \"test accuracy\": test_accuracies}).to_csv(\"./results/linear_regression_bow.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Word2vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 89.56%\n",
      "Test Accuracy: 88.56%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89      2077\n",
      "           1       0.89      0.87      0.88      2083\n",
      "\n",
      "    accuracy                           0.89      4160\n",
      "   macro avg       0.89      0.89      0.89      4160\n",
      "weighted avg       0.89      0.89      0.89      4160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "X_train_text_vectors = []\n",
    "X_test_text_vectors = []\n",
    "\n",
    "word2vec_model = preprocessor.get_word2vec_features(X_train)\n",
    "\n",
    "for text in X_train:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_train_text_vectors.append(text_vector)\n",
    "    \n",
    "for text in X_test:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_test_text_vectors.append(text_vector)\n",
    "\n",
    "X_train_text_vectors = pd.DataFrame(X_train_text_vectors)\n",
    "X_test_text_vectors = pd.DataFrame(X_test_text_vectors)\n",
    "\n",
    "lr_classifier = LogisticRegression(max_iter=1000, C=5.0, penalty='l2', solver=\"saga\", random_state=42)\n",
    "\n",
    "lr_classifier.fit(X_train_text_vectors, y_train)\n",
    "\n",
    "y_train_pred = lr_classifier.predict(X_train_text_vectors)\n",
    "y_test_pred = lr_classifier.predict(X_test_text_vectors)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.8238 - loss: 0.4092 - val_accuracy: 0.9177 - val_loss: 0.2172\n",
      "Epoch 2/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9392 - loss: 0.1714 - val_accuracy: 0.9177 - val_loss: 0.2253\n",
      "Epoch 3/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9508 - loss: 0.1472 - val_accuracy: 0.9225 - val_loss: 0.2211\n",
      "Epoch 4/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.9526 - loss: 0.1391 - val_accuracy: 0.9138 - val_loss: 0.2351\n",
      "Epoch 5/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.9519 - loss: 0.1344 - val_accuracy: 0.9261 - val_loss: 0.2231\n",
      "Epoch 6/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9564 - loss: 0.1305 - val_accuracy: 0.9210 - val_loss: 0.2185\n",
      "Epoch 7/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9543 - loss: 0.1372 - val_accuracy: 0.9258 - val_loss: 0.2187\n",
      "Epoch 8/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9594 - loss: 0.1342 - val_accuracy: 0.9231 - val_loss: 0.2284\n",
      "Epoch 9/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9550 - loss: 0.1371 - val_accuracy: 0.9150 - val_loss: 0.2279\n",
      "Epoch 10/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9551 - loss: 0.1395 - val_accuracy: 0.9177 - val_loss: 0.2314\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Accuracy: 0.9242788461538461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      2077\n",
      "           1       0.91      0.94      0.93      2083\n",
      "\n",
      "    accuracy                           0.92      4160\n",
      "   macro avg       0.92      0.92      0.92      4160\n",
      "weighted avg       0.92      0.92      0.92      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout,BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "preprocessor.train_bow_vectorizer(X_train)\n",
    "X_train_bow = preprocessor.get_bow_features(X_train)\n",
    "X_test_bow = preprocessor.get_bow_features(X_test)\n",
    "\n",
    "X_train_bow_scaled = scaler.fit_transform(X = pd.DataFrame(X_train_bow.todense()))\n",
    "X_test_bow_scaled = scaler.transform(X = pd.DataFrame(X_test_bow.todense()))\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train_bow_scaled.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer= \"Adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "training = model.fit(\n",
    "    X_train_bow_scaled,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_bow_scaled)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n",
    "\n",
    "pd.DataFrame(training.history).to_csv(\"./results/mlp_bow.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model with TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.8285 - loss: 0.3864 - val_accuracy: 0.9270 - val_loss: 0.1853\n",
      "Epoch 2/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9551 - loss: 0.1236 - val_accuracy: 0.9240 - val_loss: 0.2006\n",
      "Epoch 3/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9620 - loss: 0.1023 - val_accuracy: 0.9318 - val_loss: 0.1908\n",
      "Epoch 4/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9679 - loss: 0.0894 - val_accuracy: 0.9246 - val_loss: 0.2061\n",
      "Epoch 5/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9694 - loss: 0.0889 - val_accuracy: 0.9258 - val_loss: 0.2137\n",
      "Epoch 6/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9754 - loss: 0.0686 - val_accuracy: 0.9294 - val_loss: 0.2230\n",
      "Epoch 7/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9731 - loss: 0.0702 - val_accuracy: 0.9267 - val_loss: 0.2248\n",
      "Epoch 8/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9724 - loss: 0.0735 - val_accuracy: 0.9276 - val_loss: 0.2185\n",
      "Epoch 9/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9748 - loss: 0.0666 - val_accuracy: 0.9204 - val_loss: 0.2412\n",
      "Epoch 10/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9727 - loss: 0.0716 - val_accuracy: 0.9267 - val_loss: 0.2176\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Accuracy: 0.9213942307692308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      2077\n",
      "           1       0.93      0.91      0.92      2083\n",
      "\n",
      "    accuracy                           0.92      4160\n",
      "   macro avg       0.92      0.92      0.92      4160\n",
      "weighted avg       0.92      0.92      0.92      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout,BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "preprocessor.train_tfidf_vectorizer(X_train)\n",
    "X_train_tfidf = preprocessor.get_tfidf_features(X_train)\n",
    "X_test_tfidf = preprocessor.get_tfidf_features(X_test)\n",
    "\n",
    "X_train_tfidf_scaled = scaler.fit_transform(X = pd.DataFrame(X_train_tfidf.todense()))\n",
    "X_test_tfidf_scaled = scaler.transform(X = pd.DataFrame(X_test_tfidf.todense()))\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train_tfidf_scaled.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "training = model.fit(\n",
    "    X_train_tfidf_scaled,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_tfidf_scaled)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n",
    "\n",
    "pd.DataFrame(training.history).to_csv(\"./results/mlp_tfidf.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aibrahi/.pyenv/versions/3.11.8/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - accuracy: 0.7903 - loss: 0.4552 - val_accuracy: 0.8314 - val_loss: 0.3734\n",
      "Epoch 2/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - accuracy: 0.8721 - loss: 0.3004 - val_accuracy: 0.8456 - val_loss: 0.3368\n",
      "Epoch 3/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - accuracy: 0.8834 - loss: 0.2760 - val_accuracy: 0.8885 - val_loss: 0.2759\n",
      "Epoch 4/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - accuracy: 0.8863 - loss: 0.2617 - val_accuracy: 0.8975 - val_loss: 0.2470\n",
      "Epoch 5/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - accuracy: 0.8912 - loss: 0.2658 - val_accuracy: 0.8834 - val_loss: 0.2711\n",
      "Epoch 6/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - accuracy: 0.8971 - loss: 0.2515 - val_accuracy: 0.8975 - val_loss: 0.2496\n",
      "Epoch 7/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - accuracy: 0.8947 - loss: 0.2506 - val_accuracy: 0.8981 - val_loss: 0.2520\n",
      "Epoch 8/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - accuracy: 0.9000 - loss: 0.2436 - val_accuracy: 0.9026 - val_loss: 0.2403\n",
      "Epoch 9/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - accuracy: 0.8982 - loss: 0.2394 - val_accuracy: 0.8966 - val_loss: 0.2494\n",
      "Epoch 10/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - accuracy: 0.9014 - loss: 0.2365 - val_accuracy: 0.9044 - val_loss: 0.2335\n",
      "Epoch 11/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - accuracy: 0.9048 - loss: 0.2355 - val_accuracy: 0.8825 - val_loss: 0.2678\n",
      "Epoch 12/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - accuracy: 0.9086 - loss: 0.2264 - val_accuracy: 0.9047 - val_loss: 0.2353\n",
      "Epoch 13/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - accuracy: 0.9076 - loss: 0.2261 - val_accuracy: 0.9075 - val_loss: 0.2335\n",
      "Epoch 14/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - accuracy: 0.9046 - loss: 0.2318 - val_accuracy: 0.9041 - val_loss: 0.2428\n",
      "Epoch 15/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - accuracy: 0.9066 - loss: 0.2319 - val_accuracy: 0.8879 - val_loss: 0.2581\n",
      "Epoch 16/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - accuracy: 0.9025 - loss: 0.2327 - val_accuracy: 0.9066 - val_loss: 0.2330\n",
      "Epoch 17/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - accuracy: 0.9100 - loss: 0.2205 - val_accuracy: 0.8990 - val_loss: 0.2393\n",
      "Epoch 18/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - accuracy: 0.9119 - loss: 0.2164 - val_accuracy: 0.8900 - val_loss: 0.2803\n",
      "Epoch 19/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - accuracy: 0.9068 - loss: 0.2282 - val_accuracy: 0.9123 - val_loss: 0.2187\n",
      "Epoch 20/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - accuracy: 0.9099 - loss: 0.2193 - val_accuracy: 0.9135 - val_loss: 0.2183\n",
      "Epoch 21/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - accuracy: 0.9066 - loss: 0.2225 - val_accuracy: 0.9026 - val_loss: 0.2403\n",
      "Epoch 22/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - accuracy: 0.9089 - loss: 0.2276 - val_accuracy: 0.9123 - val_loss: 0.2301\n",
      "Epoch 23/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - accuracy: 0.9088 - loss: 0.2280 - val_accuracy: 0.9081 - val_loss: 0.2296\n",
      "Epoch 24/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - accuracy: 0.9139 - loss: 0.2035 - val_accuracy: 0.8912 - val_loss: 0.2530\n",
      "Epoch 25/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - accuracy: 0.9131 - loss: 0.2204 - val_accuracy: 0.9120 - val_loss: 0.2269\n",
      "Epoch 26/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - accuracy: 0.9130 - loss: 0.2179 - val_accuracy: 0.8891 - val_loss: 0.2691\n",
      "Epoch 27/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - accuracy: 0.9047 - loss: 0.2256 - val_accuracy: 0.9111 - val_loss: 0.2251\n",
      "Epoch 28/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - accuracy: 0.9183 - loss: 0.2001 - val_accuracy: 0.8633 - val_loss: 0.3176\n",
      "Epoch 29/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - accuracy: 0.9143 - loss: 0.2137 - val_accuracy: 0.8861 - val_loss: 0.2630\n",
      "Epoch 30/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 0.9156 - loss: 0.2090 - val_accuracy: 0.8900 - val_loss: 0.2633\n",
      "Epoch 31/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - accuracy: 0.9164 - loss: 0.2094 - val_accuracy: 0.9072 - val_loss: 0.2221\n",
      "Epoch 32/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - accuracy: 0.9168 - loss: 0.2036 - val_accuracy: 0.8531 - val_loss: 0.3803\n",
      "Epoch 33/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - accuracy: 0.9057 - loss: 0.2281 - val_accuracy: 0.9153 - val_loss: 0.2216\n",
      "Epoch 34/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9141 - loss: 0.2149 - val_accuracy: 0.9150 - val_loss: 0.2171\n",
      "Epoch 35/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - accuracy: 0.9170 - loss: 0.1993 - val_accuracy: 0.9044 - val_loss: 0.2480\n",
      "Epoch 36/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - accuracy: 0.9203 - loss: 0.2100 - val_accuracy: 0.9126 - val_loss: 0.2175\n",
      "Epoch 37/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - accuracy: 0.9122 - loss: 0.2079 - val_accuracy: 0.9002 - val_loss: 0.2415\n",
      "Epoch 38/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - accuracy: 0.9168 - loss: 0.2052 - val_accuracy: 0.9008 - val_loss: 0.2426\n",
      "Epoch 39/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - accuracy: 0.9173 - loss: 0.2004 - val_accuracy: 0.9141 - val_loss: 0.2194\n",
      "Epoch 40/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - accuracy: 0.9148 - loss: 0.1953 - val_accuracy: 0.9120 - val_loss: 0.2130\n",
      "Epoch 41/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - accuracy: 0.9185 - loss: 0.1976 - val_accuracy: 0.9041 - val_loss: 0.2376\n",
      "Epoch 42/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - accuracy: 0.9203 - loss: 0.2018 - val_accuracy: 0.8681 - val_loss: 0.3162\n",
      "Epoch 43/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - accuracy: 0.9210 - loss: 0.1959 - val_accuracy: 0.9132 - val_loss: 0.2175\n",
      "Epoch 44/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - accuracy: 0.9154 - loss: 0.2045 - val_accuracy: 0.9132 - val_loss: 0.2178\n",
      "Epoch 45/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - accuracy: 0.9191 - loss: 0.1946 - val_accuracy: 0.8936 - val_loss: 0.2589\n",
      "Epoch 46/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - accuracy: 0.9185 - loss: 0.2070 - val_accuracy: 0.9059 - val_loss: 0.2260\n",
      "Epoch 47/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.9203 - loss: 0.1966 - val_accuracy: 0.9132 - val_loss: 0.2334\n",
      "Epoch 48/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 0.9180 - loss: 0.2018 - val_accuracy: 0.8987 - val_loss: 0.2439\n",
      "Epoch 49/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.9213 - loss: 0.1993 - val_accuracy: 0.8678 - val_loss: 0.3286\n",
      "Epoch 50/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - accuracy: 0.9148 - loss: 0.2010 - val_accuracy: 0.9153 - val_loss: 0.2147\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310us/step\n",
      "Accuracy: 0.9088942307692308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91      2077\n",
      "           1       0.90      0.93      0.91      2083\n",
      "\n",
      "    accuracy                           0.91      4160\n",
      "   macro avg       0.91      0.91      0.91      4160\n",
      "weighted avg       0.91      0.91      0.91      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "X_train_text_vectors = []\n",
    "X_test_text_vectors = []\n",
    "\n",
    "word2vec_model = preprocessor.get_word2vec_features(X_train)\n",
    "\n",
    "for text in X_train:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_train_text_vectors.append(text_vector)\n",
    "    \n",
    "for text in X_test:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_test_text_vectors.append(text_vector)\n",
    "\n",
    "X_train_text_vectors = pd.DataFrame(X_train_text_vectors)\n",
    "X_test_text_vectors = pd.DataFrame(X_test_text_vectors)\n",
    "\n",
    "X_train_word2Vec = X_train_text_vectors.to_numpy()\n",
    "X_test_word2Vec = X_test_text_vectors.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train_word2Vec.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train_word2Vec,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_word2Vec) \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 460ms/step - accuracy: 0.7829 - loss: 0.4229 - val_accuracy: 0.9303 - val_loss: 0.1759\n",
      "Epoch 2/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 410ms/step - accuracy: 0.9734 - loss: 0.0802 - val_accuracy: 0.9351 - val_loss: 0.1999\n",
      "Epoch 3/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 384ms/step - accuracy: 0.9950 - loss: 0.0182 - val_accuracy: 0.9264 - val_loss: 0.2844\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step\n",
      "Accuracy: 0.9305288461538461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93      2077\n",
      "           1       0.97      0.89      0.93      2083\n",
      "\n",
      "    accuracy                           0.93      4160\n",
      "   macro avg       0.93      0.93      0.93      4160\n",
      "weighted avg       0.93      0.93      0.93      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "tokenizer = Tokenizer()\n",
    "max_sequence_length = 200\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "X_train_sequences = pd.DataFrame(X_train_sequences)\n",
    "X_test_sequences = pd.DataFrame(X_test_sequences)\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))  \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train_sequences,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_sequences)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 120ms/step - accuracy: 0.7196 - loss: 0.5299 - val_accuracy: 0.8915 - val_loss: 0.2555\n",
      "Epoch 2/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 118ms/step - accuracy: 0.9244 - loss: 0.2158 - val_accuracy: 0.9053 - val_loss: 0.2363\n",
      "Epoch 3/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 119ms/step - accuracy: 0.9759 - loss: 0.0871 - val_accuracy: 0.9198 - val_loss: 0.2324\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step\n",
      "Accuracy: 0.9225961538461539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92      2077\n",
      "           1       0.94      0.90      0.92      2083\n",
      "\n",
      "    accuracy                           0.92      4160\n",
      "   macro avg       0.92      0.92      0.92      4160\n",
      "weighted avg       0.92      0.92      0.92      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "tokenizer = Tokenizer()\n",
    "max_sequence_length = 200\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "X_train_sequences = pd.DataFrame(X_train_sequences)\n",
    "X_test_sequences = pd.DataFrame(X_test_sequences)\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(5, return_sequences=False)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dropout(0.3))  \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer = \"adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train_sequences,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_sequences)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train.tolist(), truncation = True, padding = True, max_length = 300, return_tensors = \"tf\")\n",
    "X_test_tokenized = tokenizer(X_test.tolist(), truncation = True, padding = True, max_length = 300, return_tensors = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2)\n",
    "\n",
    "# Freeze all layers except classifier\n",
    "for layer in model.layers:\n",
    "    if layer.name != \"classifier\":\n",
    "        layer.trainable = False\n",
    "\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate = 0.00001)\n",
    "model.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=['accuracy'])\n",
    "\n",
    "training = model.fit(\n",
    "    X_train_tokenized,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(training.history).to_csv(\"./results/bert.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 97s 749ms/step\n",
      "Results on Test Data:\n",
      "Accuracy: 0.4206730769230769\n",
      "Precision: 0.3942262643853256\n",
      "Recall: 0.4203147611947233\n",
      "F1 Score: 0.38236740760892596\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.17      0.23      2077\n",
      "           1       0.45      0.67      0.54      2083\n",
      "\n",
      "    accuracy                           0.42      4160\n",
      "   macro avg       0.39      0.42      0.38      4160\n",
      "weighted avg       0.39      0.42      0.38      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "y_test_pred_logits = model.predict(X_test_tokenized).logits\n",
    "y_test_pred = np.argmax(y_test_pred_logits, axis = 1)  \n",
    "print(\"Results on Test Data:\")\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print()\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
