{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        if isinstance(text, float):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        # https://medium.com/@siddharthgov01/regular-expressions-from-a-za-z-88cf9cf0abac\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        return text\n",
    "    \n",
    "\n",
    "    # https://pages.github.rpi.edu/kuruzj/website_introml_rpi/notebooks/08-intro-nlp/03-scikit-learn-text.html#bag-of-words-using-scikit-learn\n",
    "    def get_bow_features(self, texts, max_features=5000):\n",
    "        vectorizer = CountVectorizer(max_features=max_features)\n",
    "        return vectorizer.fit_transform(texts)\n",
    "\n",
    "    # https://pages.github.rpi.edu/kuruzj/website_introml_rpi/notebooks/08-intro-nlp/03-scikit-learn-text.html#tf-idf-encoding\n",
    "    def get_tfidf_features(self, texts, max_features=5000):\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "        return vectorizer.fit_transform(texts)\n",
    " \n",
    "    # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    def get_word2vec_features(self, texts, vector_size=100, window=5, min_count=1, workers=4):\n",
    "        processed_texts = []\n",
    "        for text in texts:\n",
    "            if isinstance(text, str):\n",
    "                cleaned = self.clean_text(text)\n",
    "                tokens = cleaned.split()\n",
    "                processed_texts.append(tokens)\n",
    "\n",
    "        model = Word2Vec(\n",
    "            sentences=processed_texts,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            workers=workers\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def get_text_vector(self, text, word2vec_model):\n",
    "        tokens = self.clean_text(text).split()\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in word2vec_model.wv:\n",
    "                vectors.append(word2vec_model.wv[token])\n",
    "                \n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with TF-IDF Word Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number Iterations: 1\n",
      "Train Accuracy: 86.63%\n",
      "Test Accuracy: 63.34%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.67      0.64      2077\n",
      "           1       0.64      0.60      0.62      2083\n",
      "\n",
      "    accuracy                           0.63      4160\n",
      "   macro avg       0.63      0.63      0.63      4160\n",
      "weighted avg       0.63      0.63      0.63      4160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "scaler = StandardScaler()\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "X_train_tfidf = preprocessor.get_tfidf_features(X_train)\n",
    "X_test_tfidf = preprocessor.get_tfidf_features(X_test)\n",
    "\n",
    "X_train_tfidf_scaled = scaler.fit_transform(X = pd.DataFrame(X_train_tfidf.todense()))\n",
    "X_test_tfidf_scaled = scaler.fit_transform(X = pd.DataFrame(X_test_tfidf.todense()))\n",
    "best_iteration = None\n",
    "best_train_accuracy = None\n",
    "best_test_accuracy = None\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "for i in range(1, 51):\n",
    "    lr_classifier = LogisticRegression(max_iter=i, C=5.0, penalty='l2', random_state=42 )\n",
    "\n",
    "\n",
    "    lr_classifier.fit(X_train_tfidf_scaled, y_train)\n",
    "\n",
    "\n",
    "    y_train_pred = lr_classifier.predict(X_train_tfidf_scaled)\n",
    "    y_test_pred = lr_classifier.predict(X_test_tfidf_scaled)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    if best_test_accuracy == None or test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        best_train_accuracy = train_accuracy\n",
    "        best_iteration = i\n",
    "\n",
    "print(\"Best Number Iterations:\", best_iteration)\n",
    "print(f\"Train Accuracy: {best_train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {best_test_accuracy * 100:.2f}%\")\n",
    "\n",
    "lr_classifier = LogisticRegression(max_iter=best_iteration, C=5.0, penalty='l2', random_state=42 )\n",
    "lr_classifier.fit(X_train_tfidf_scaled, y_train)\n",
    "y_test_pred = lr_classifier.predict(X_test_tfidf_scaled)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iterations = list(range(1, 51))\n",
    "plt.plot(iterations, train_accuracies, label = \"train\")\n",
    "plt.plot(iterations, test_accuracies, label = \"test\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Max Iterations\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 93.08%\n",
      "Test Accuracy: 65.77%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.70      2077\n",
      "           1       0.71      0.53      0.61      2083\n",
      "\n",
      "    accuracy                           0.66      4160\n",
      "   macro avg       0.67      0.66      0.65      4160\n",
      "weighted avg       0.67      0.66      0.65      4160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "scaler = StandardScaler()\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "X_train_bow = preprocessor.get_bow_features(X_train)\n",
    "X_test_bow = preprocessor.get_bow_features (X_test)\n",
    "\n",
    "X_train_bow_scaled = scaler.fit_transform(X = pd.DataFrame(X_train_bow.todense()))\n",
    "X_test_bow_scaled = scaler.fit_transform(X = pd.DataFrame(X_test_bow.todense()))\n",
    "\n",
    "best_iteration = None\n",
    "best_train_accuracy = None\n",
    "best_test_accuracy = None\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for i in range(1, 51):\n",
    "    lr_classifier = LogisticRegression(max_iter=i, C=5.0, penalty='l2', random_state=42 )\n",
    "\n",
    "\n",
    "    lr_classifier.fit(X_train_bow_scaled, y_train)\n",
    "\n",
    "\n",
    "    y_train_pred = lr_classifier.predict(X_train_bow_scaled)\n",
    "    y_test_pred = lr_classifier.predict(X_test_bow_scaled)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    # print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    # print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    if best_test_accuracy == None or test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        best_train_accuracy = train_accuracy\n",
    "        best_iteration = i\n",
    "\n",
    "print(\"Best Number Iterations:\", best_iteration)\n",
    "print(f\"Train Accuracy: {best_train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {best_test_accuracy * 100:.2f}%\") \n",
    "\n",
    "lr_classifier = LogisticRegression(max_iter=20, C=5.0, penalty='l2', random_state=42 )\n",
    "\n",
    "\n",
    "\n",
    "lr_classifier.fit(X_train_bow, y_train)\n",
    "\n",
    "\n",
    "y_train_pred = lr_classifier.predict(X_train_bow)\n",
    "y_test_pred = lr_classifier.predict(X_test_bow)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Word2vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 89.56%\n",
      "Test Accuracy: 88.56%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89      2077\n",
      "           1       0.89      0.87      0.88      2083\n",
      "\n",
      "    accuracy                           0.89      4160\n",
      "   macro avg       0.89      0.89      0.89      4160\n",
      "weighted avg       0.89      0.89      0.89      4160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "X_train_text_vectors = []\n",
    "X_test_text_vectors = []\n",
    "\n",
    "word2vec_model = preprocessor.get_word2vec_features(X_train)\n",
    "\n",
    "for text in X_train:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_train_text_vectors.append(text_vector)\n",
    "    \n",
    "for text in X_test:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_test_text_vectors.append(text_vector)\n",
    "\n",
    "X_train_text_vectors = pd.DataFrame(X_train_text_vectors)\n",
    "X_test_text_vectors = pd.DataFrame(X_test_text_vectors)\n",
    "\n",
    "lr_classifier = LogisticRegression(max_iter=1000, C=5.0, penalty='l2', solver=\"saga\", random_state=42)\n",
    "\n",
    "lr_classifier.fit(X_train_text_vectors, y_train)\n",
    "\n",
    "y_train_pred = lr_classifier.predict(X_train_text_vectors)\n",
    "y_test_pred = lr_classifier.predict(X_test_text_vectors)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aibrahi/.pyenv/versions/3.11.8/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8366 - loss: 0.3610 - val_accuracy: 0.8825 - val_loss: 0.2824\n",
      "Epoch 2/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9556 - loss: 0.1224 - val_accuracy: 0.9029 - val_loss: 0.2501\n",
      "Epoch 3/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9677 - loss: 0.0879 - val_accuracy: 0.9348 - val_loss: 0.1763\n",
      "Epoch 4/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9754 - loss: 0.0642 - val_accuracy: 0.9468 - val_loss: 0.1587\n",
      "Epoch 5/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9821 - loss: 0.0490 - val_accuracy: 0.9432 - val_loss: 0.1698\n",
      "Epoch 6/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9849 - loss: 0.0441 - val_accuracy: 0.9375 - val_loss: 0.1808\n",
      "Epoch 7/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0398 - val_accuracy: 0.9453 - val_loss: 0.1668\n",
      "Epoch 8/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9849 - loss: 0.0439 - val_accuracy: 0.9483 - val_loss: 0.1645\n",
      "Epoch 9/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9860 - loss: 0.0410 - val_accuracy: 0.9453 - val_loss: 0.1769\n",
      "Epoch 10/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9870 - loss: 0.0363 - val_accuracy: 0.9447 - val_loss: 0.1798\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "Accuracy: 0.5550480769230769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.30      0.40      2077\n",
      "           1       0.54      0.81      0.65      2083\n",
      "\n",
      "    accuracy                           0.56      4160\n",
      "   macro avg       0.57      0.55      0.52      4160\n",
      "weighted avg       0.57      0.56      0.52      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.layers import Dense, Dropout,BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "X_train_bow = preprocessor.get_bow_features(X_train).toarray()\n",
    "X_test_bow = preprocessor.get_bow_features (X_test).toarray()\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train_bow.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer= \"Adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train_bow,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_bow)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model with TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aibrahi/.pyenv/versions/3.11.8/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8576 - loss: 0.3205 - val_accuracy: 0.9390 - val_loss: 0.2511\n",
      "Epoch 2/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 0.0743 - val_accuracy: 0.9372 - val_loss: 0.1565\n",
      "Epoch 3/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9859 - loss: 0.0471 - val_accuracy: 0.9390 - val_loss: 0.1895\n",
      "Epoch 4/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9913 - loss: 0.0258 - val_accuracy: 0.9366 - val_loss: 0.2255\n",
      "Epoch 5/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.0223 - val_accuracy: 0.9393 - val_loss: 0.2184\n",
      "Epoch 6/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9899 - loss: 0.0250 - val_accuracy: 0.9270 - val_loss: 0.2864\n",
      "Epoch 7/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9910 - loss: 0.0241 - val_accuracy: 0.9306 - val_loss: 0.2883\n",
      "Epoch 8/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0204 - val_accuracy: 0.9405 - val_loss: 0.2442\n",
      "Epoch 9/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9936 - loss: 0.0179 - val_accuracy: 0.9399 - val_loss: 0.2438\n",
      "Epoch 10/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9901 - loss: 0.0272 - val_accuracy: 0.9396 - val_loss: 0.2417\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488us/step\n",
      "Accuracy: 0.5759615384615384\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.39      0.48      2077\n",
      "           1       0.56      0.76      0.64      2083\n",
      "\n",
      "    accuracy                           0.58      4160\n",
      "   macro avg       0.59      0.58      0.56      4160\n",
      "weighted avg       0.59      0.58      0.56      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.layers import Dense, Dropout,BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "\n",
    "X_train_tfidf = preprocessor.get_tfidf_features(X_train).toarray()\n",
    "X_test_tfidf = preprocessor.get_tfidf_features(X_test).toarray()\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train_tfidf,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_tfidf)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aibrahi/.pyenv/versions/3.11.8/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - accuracy: 0.7903 - loss: 0.4552 - val_accuracy: 0.8314 - val_loss: 0.3734\n",
      "Epoch 2/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - accuracy: 0.8721 - loss: 0.3004 - val_accuracy: 0.8456 - val_loss: 0.3368\n",
      "Epoch 3/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - accuracy: 0.8834 - loss: 0.2760 - val_accuracy: 0.8885 - val_loss: 0.2759\n",
      "Epoch 4/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - accuracy: 0.8863 - loss: 0.2617 - val_accuracy: 0.8975 - val_loss: 0.2470\n",
      "Epoch 5/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - accuracy: 0.8912 - loss: 0.2658 - val_accuracy: 0.8834 - val_loss: 0.2711\n",
      "Epoch 6/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - accuracy: 0.8971 - loss: 0.2515 - val_accuracy: 0.8975 - val_loss: 0.2496\n",
      "Epoch 7/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - accuracy: 0.8947 - loss: 0.2506 - val_accuracy: 0.8981 - val_loss: 0.2520\n",
      "Epoch 8/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - accuracy: 0.9000 - loss: 0.2436 - val_accuracy: 0.9026 - val_loss: 0.2403\n",
      "Epoch 9/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - accuracy: 0.8982 - loss: 0.2394 - val_accuracy: 0.8966 - val_loss: 0.2494\n",
      "Epoch 10/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - accuracy: 0.9014 - loss: 0.2365 - val_accuracy: 0.9044 - val_loss: 0.2335\n",
      "Epoch 11/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - accuracy: 0.9048 - loss: 0.2355 - val_accuracy: 0.8825 - val_loss: 0.2678\n",
      "Epoch 12/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - accuracy: 0.9086 - loss: 0.2264 - val_accuracy: 0.9047 - val_loss: 0.2353\n",
      "Epoch 13/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - accuracy: 0.9076 - loss: 0.2261 - val_accuracy: 0.9075 - val_loss: 0.2335\n",
      "Epoch 14/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - accuracy: 0.9046 - loss: 0.2318 - val_accuracy: 0.9041 - val_loss: 0.2428\n",
      "Epoch 15/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - accuracy: 0.9066 - loss: 0.2319 - val_accuracy: 0.8879 - val_loss: 0.2581\n",
      "Epoch 16/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - accuracy: 0.9025 - loss: 0.2327 - val_accuracy: 0.9066 - val_loss: 0.2330\n",
      "Epoch 17/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - accuracy: 0.9100 - loss: 0.2205 - val_accuracy: 0.8990 - val_loss: 0.2393\n",
      "Epoch 18/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - accuracy: 0.9119 - loss: 0.2164 - val_accuracy: 0.8900 - val_loss: 0.2803\n",
      "Epoch 19/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - accuracy: 0.9068 - loss: 0.2282 - val_accuracy: 0.9123 - val_loss: 0.2187\n",
      "Epoch 20/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - accuracy: 0.9099 - loss: 0.2193 - val_accuracy: 0.9135 - val_loss: 0.2183\n",
      "Epoch 21/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - accuracy: 0.9066 - loss: 0.2225 - val_accuracy: 0.9026 - val_loss: 0.2403\n",
      "Epoch 22/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - accuracy: 0.9089 - loss: 0.2276 - val_accuracy: 0.9123 - val_loss: 0.2301\n",
      "Epoch 23/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - accuracy: 0.9088 - loss: 0.2280 - val_accuracy: 0.9081 - val_loss: 0.2296\n",
      "Epoch 24/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - accuracy: 0.9139 - loss: 0.2035 - val_accuracy: 0.8912 - val_loss: 0.2530\n",
      "Epoch 25/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - accuracy: 0.9131 - loss: 0.2204 - val_accuracy: 0.9120 - val_loss: 0.2269\n",
      "Epoch 26/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - accuracy: 0.9130 - loss: 0.2179 - val_accuracy: 0.8891 - val_loss: 0.2691\n",
      "Epoch 27/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - accuracy: 0.9047 - loss: 0.2256 - val_accuracy: 0.9111 - val_loss: 0.2251\n",
      "Epoch 28/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - accuracy: 0.9183 - loss: 0.2001 - val_accuracy: 0.8633 - val_loss: 0.3176\n",
      "Epoch 29/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - accuracy: 0.9143 - loss: 0.2137 - val_accuracy: 0.8861 - val_loss: 0.2630\n",
      "Epoch 30/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 0.9156 - loss: 0.2090 - val_accuracy: 0.8900 - val_loss: 0.2633\n",
      "Epoch 31/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - accuracy: 0.9164 - loss: 0.2094 - val_accuracy: 0.9072 - val_loss: 0.2221\n",
      "Epoch 32/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - accuracy: 0.9168 - loss: 0.2036 - val_accuracy: 0.8531 - val_loss: 0.3803\n",
      "Epoch 33/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - accuracy: 0.9057 - loss: 0.2281 - val_accuracy: 0.9153 - val_loss: 0.2216\n",
      "Epoch 34/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9141 - loss: 0.2149 - val_accuracy: 0.9150 - val_loss: 0.2171\n",
      "Epoch 35/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - accuracy: 0.9170 - loss: 0.1993 - val_accuracy: 0.9044 - val_loss: 0.2480\n",
      "Epoch 36/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - accuracy: 0.9203 - loss: 0.2100 - val_accuracy: 0.9126 - val_loss: 0.2175\n",
      "Epoch 37/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - accuracy: 0.9122 - loss: 0.2079 - val_accuracy: 0.9002 - val_loss: 0.2415\n",
      "Epoch 38/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - accuracy: 0.9168 - loss: 0.2052 - val_accuracy: 0.9008 - val_loss: 0.2426\n",
      "Epoch 39/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - accuracy: 0.9173 - loss: 0.2004 - val_accuracy: 0.9141 - val_loss: 0.2194\n",
      "Epoch 40/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - accuracy: 0.9148 - loss: 0.1953 - val_accuracy: 0.9120 - val_loss: 0.2130\n",
      "Epoch 41/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - accuracy: 0.9185 - loss: 0.1976 - val_accuracy: 0.9041 - val_loss: 0.2376\n",
      "Epoch 42/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - accuracy: 0.9203 - loss: 0.2018 - val_accuracy: 0.8681 - val_loss: 0.3162\n",
      "Epoch 43/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - accuracy: 0.9210 - loss: 0.1959 - val_accuracy: 0.9132 - val_loss: 0.2175\n",
      "Epoch 44/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - accuracy: 0.9154 - loss: 0.2045 - val_accuracy: 0.9132 - val_loss: 0.2178\n",
      "Epoch 45/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - accuracy: 0.9191 - loss: 0.1946 - val_accuracy: 0.8936 - val_loss: 0.2589\n",
      "Epoch 46/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - accuracy: 0.9185 - loss: 0.2070 - val_accuracy: 0.9059 - val_loss: 0.2260\n",
      "Epoch 47/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.9203 - loss: 0.1966 - val_accuracy: 0.9132 - val_loss: 0.2334\n",
      "Epoch 48/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 0.9180 - loss: 0.2018 - val_accuracy: 0.8987 - val_loss: 0.2439\n",
      "Epoch 49/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.9213 - loss: 0.1993 - val_accuracy: 0.8678 - val_loss: 0.3286\n",
      "Epoch 50/50\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - accuracy: 0.9148 - loss: 0.2010 - val_accuracy: 0.9153 - val_loss: 0.2147\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310us/step\n",
      "Accuracy: 0.9088942307692308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91      2077\n",
      "           1       0.90      0.93      0.91      2083\n",
      "\n",
      "    accuracy                           0.91      4160\n",
      "   macro avg       0.91      0.91      0.91      4160\n",
      "weighted avg       0.91      0.91      0.91      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "X_train_text_vectors = []\n",
    "X_test_text_vectors = []\n",
    "\n",
    "word2vec_model = preprocessor.get_word2vec_features(X_train)\n",
    "\n",
    "for text in X_train:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_train_text_vectors.append(text_vector)\n",
    "    \n",
    "for text in X_test:\n",
    "    text_vector = preprocessor.get_text_vector(text, word2vec_model)\n",
    "    X_test_text_vectors.append(text_vector)\n",
    "\n",
    "X_train_text_vectors = pd.DataFrame(X_train_text_vectors)\n",
    "X_test_text_vectors = pd.DataFrame(X_test_text_vectors)\n",
    "\n",
    "X_train_word2Vec = X_train_text_vectors.to_numpy()\n",
    "X_test_word2Vec = X_test_text_vectors.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train_word2Vec.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train_word2Vec,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_word2Vec) \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 460ms/step - accuracy: 0.7829 - loss: 0.4229 - val_accuracy: 0.9303 - val_loss: 0.1759\n",
      "Epoch 2/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 410ms/step - accuracy: 0.9734 - loss: 0.0802 - val_accuracy: 0.9351 - val_loss: 0.1999\n",
      "Epoch 3/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 384ms/step - accuracy: 0.9950 - loss: 0.0182 - val_accuracy: 0.9264 - val_loss: 0.2844\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step\n",
      "Accuracy: 0.9305288461538461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93      2077\n",
      "           1       0.97      0.89      0.93      2083\n",
      "\n",
      "    accuracy                           0.93      4160\n",
      "   macro avg       0.93      0.93      0.93      4160\n",
      "weighted avg       0.93      0.93      0.93      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "tokenizer = Tokenizer()\n",
    "max_sequence_length = 200\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "X_train_sequences = pd.DataFrame(X_train_sequences)\n",
    "X_test_sequences = pd.DataFrame(X_test_sequences)\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))  \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train_sequences,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_sequences)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 120ms/step - accuracy: 0.7196 - loss: 0.5299 - val_accuracy: 0.8915 - val_loss: 0.2555\n",
      "Epoch 2/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 118ms/step - accuracy: 0.9244 - loss: 0.2158 - val_accuracy: 0.9053 - val_loss: 0.2363\n",
      "Epoch 3/3\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 119ms/step - accuracy: 0.9759 - loss: 0.0871 - val_accuracy: 0.9198 - val_loss: 0.2324\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step\n",
      "Accuracy: 0.9225961538461539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92      2077\n",
      "           1       0.94      0.90      0.92      2083\n",
      "\n",
      "    accuracy                           0.92      4160\n",
      "   macro avg       0.92      0.92      0.92      4160\n",
      "weighted avg       0.92      0.92      0.92      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# Source https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "tokenizer = Tokenizer()\n",
    "max_sequence_length = 200\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "X_train_sequences = pd.DataFrame(X_train_sequences)\n",
    "X_test_sequences = pd.DataFrame(X_test_sequences)\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "y_train_encoded = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(5, return_sequences=False)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dropout(0.3))  \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer = \"adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train_sequences,\n",
    "    y_train_encoded,\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict(X_test_sequences)  \n",
    "y_pred = np.argmax(y_pred_prob, axis=1)   \n",
    "y_test_labels = np.argmax(y_test_encoded, axis=1) \n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test_labels, y_pred))\n",
    "print(classification_report(y_test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = train_df['text'].apply(preprocessor.clean_text)\n",
    "train_labels = train_df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train.tolist(), truncation = True, padding = True, max_length = 300, return_tensors = \"tf\")\n",
    "X_test_tokenized = tokenizer(X_test.tolist(), truncation = True, padding = True, max_length = 300, return_tensors = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2)\n",
    "\n",
    "# Freeze all layers except classifier\n",
    "for layer in model.layers:\n",
    "    if layer.name != \"classifier\":\n",
    "        layer.trainable = False\n",
    "\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate = 0.00001)\n",
    "model.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=['accuracy'])\n",
    "\n",
    "training = model.fit(\n",
    "    X_train_tokenized,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(training.history).to_csv(\"./train_results/bert.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 97s 749ms/step\n",
      "Results on Test Data:\n",
      "Accuracy: 0.4206730769230769\n",
      "Precision: 0.3942262643853256\n",
      "Recall: 0.4203147611947233\n",
      "F1 Score: 0.38236740760892596\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.17      0.23      2077\n",
      "           1       0.45      0.67      0.54      2083\n",
      "\n",
      "    accuracy                           0.42      4160\n",
      "   macro avg       0.39      0.42      0.38      4160\n",
      "weighted avg       0.39      0.42      0.38      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "y_test_pred_logits = model.predict(X_test_tokenized).logits\n",
    "y_test_pred = np.argmax(y_test_pred_logits, axis = 1)  \n",
    "print(\"Results on Test Data:\")\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred, average = \"macro\", zero_division = 0))\n",
    "print()\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
